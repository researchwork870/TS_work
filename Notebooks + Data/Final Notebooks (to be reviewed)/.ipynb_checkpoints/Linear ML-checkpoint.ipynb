{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0e7894",
   "metadata": {},
   "source": [
    "# LINEAR MACHINE LEARNING TECHNIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd \n",
    "import math\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a43ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use this notebook for univarate time series analysis :-\n",
    "# 1) The primary requirement is not to have missing values or categorial(string) data for time_dependent variable \n",
    "#    and time_column.\n",
    "# 2) This cell requires information on file_name (only csv), time_dependent_variable, time_column, date_time format (frmt)\n",
    "#    and resample grain(X). After filling the required information correctly, you can run all the cells (Cell ---> Run All)\n",
    "# 3) Example :-\n",
    "#   file_name               = \"JetRail Avg Hourly Traffic Data - 2012-2013.csv\"\n",
    "#   time_dependent_variable = \"Count\"    (column name in your dataset)\n",
    "#   time_column             = \"Datetime\" (column name in your dataset)\n",
    "#   frmt                    = \"%Y-%m-%d\"\n",
    "#   X                       = \"D\" \n",
    "\n",
    "file_name = \"cta_ridership.csv\"\n",
    "time_dependent_variable = \"total_rides\"\n",
    "time_column = \"service_date\"\n",
    "frmt =  '%Y-%m-%d'\n",
    "X = \"D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02669229",
   "metadata": {},
   "source": [
    "### Reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f69115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(time_column, file_name, frmt='%Y-%m-%d %H:%M:%S', X= \"D\"):\n",
    "    df = pd.read_csv(file_name, parse_dates= True)\n",
    "    df = df[[time_column,time_dependent_variable]]\n",
    "    df[time_column] = pd.to_datetime(df[time_column],format=frmt) \n",
    "    df.index = df[time_column]\n",
    "    df = df.resample(X).mean()\n",
    "    df.reset_index(inplace= True)\n",
    "    return df\n",
    "df = data(time_column, file_name, frmt, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310efb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c13d0",
   "metadata": {},
   "source": [
    "## Defining the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682be2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_evaluation_metrics_func(y_true, y_pred):\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print('Evaluation metric results:-')\n",
    "    print(f'MSE is : {metrics.mean_squared_error(y_true, y_pred)}')\n",
    "    print(f'MAE is : {metrics.mean_absolute_error(y_true, y_pred)}')\n",
    "    print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}')\n",
    "    print(f'MAPE is : {mean_absolute_percentage_error(y_true,y_pred)}') #WMAPE\n",
    "    print(f'R2 is : {metrics.r2_score(y_true, y_pred)}',end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf96465",
   "metadata": {},
   "source": [
    "## Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22142c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(train.index, train[time_dependent_variable], label='Train')\n",
    "    plt.plot(test.index,test[time_dependent_variable], label='Test')\n",
    "    plt.plot(test.index,pred, label= 'forecast')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('forecast')\n",
    "    plt.show()\n",
    "    timeseries_evaluation_metrics_func(test_Y,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9246c7",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "To use supervised machine learning, we need input and output variables. We can create features from a univariate time series in the following ways :-\n",
    "- Date Time Features : These are components of the time step itself for each observation.\n",
    "- Lag Features : These are values at prior time steps.\n",
    "- Window Features : These are a summary of values over a fixed window of prior time steps.\n",
    "\n",
    "let's try and explore each one of them...\n",
    "\n",
    "#### 1) Date Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_features(df):\n",
    "    df['year'] = df[time_column].dt.year\n",
    "    df['quarter'] = df[time_column].dt.quarter\n",
    "    df['month'] = df[time_column].dt.month\n",
    "    df['week_day'] = df[time_column].dt.weekday\n",
    "    return df\n",
    "df_date_features = date_features(df)[[\"year\",\"quarter\",\"month\",\"week_day\",time_dependent_variable]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7db94a",
   "metadata": {},
   "source": [
    "#### 2) Lag Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33376044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag_features = pd.DataFrame()\n",
    "\n",
    "def lag_features(n):\n",
    "    df_lag_features[\"lag_{}\".format(n)] = df[time_dependent_variable].shift(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b019b3",
   "metadata": {},
   "source": [
    "Let's take lag = 1 and lag = 2 as features. It totally depends on your problem......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want a different lag k just write lag_features(k) and the function would automatically do it for you..\n",
    "lag_features(1)\n",
    "lag_features(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518296e",
   "metadata": {},
   "source": [
    "#### 3) Window Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window_features = pd.DataFrame()\n",
    "\n",
    "def rolling_mean(n):\n",
    "    df_window_features[\"Rolling_mean_{}\".format(n)] = df[time_dependent_variable].shift(1).rolling(window = n).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39627dd",
   "metadata": {},
   "source": [
    "Let's take rolling mean of 2 and 5. It totally depends on your problem....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want a different rolling mean k just write rolling_mean(k) and the function would automatically do it for you..\n",
    "rolling_mean(2)\n",
    "rolling_mean(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b388a",
   "metadata": {},
   "source": [
    "#### Combining the table with features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0530c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(pd.merge(df_date_features,df_lag_features,left_index=True,right_index=True),df_window_features,left_index=True, right_index=True)\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8d4a8",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test using (you can use any one of them) :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This splits the data into train and test using default split_size = 0.7\n",
    "def train_test_split_perc(df, split= 0.7):\n",
    "    total_size=len(df)\n",
    "    train_size=math.floor(split*total_size) #(70% Dataset)\n",
    "    train = df.head(train_size)\n",
    "    test  = df.tail(len(df) - train_size)\n",
    "    return train,test\n",
    "    \n",
    "train,test = train_test_split_perc(df, split= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train[time_dependent_variable]\n",
    "train_X = train[[i for i in df.columns if i != time_dependent_variable]]\n",
    "test_Y = test[time_dependent_variable]\n",
    "test_X = test[[i for i in df.columns if i != time_dependent_variable]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd224045",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_Y) == len(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f5e08",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data. There are many methods to transform your data. For more information,https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b51e7",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9e076",
   "metadata": {},
   "source": [
    "### 1) Linear Regression \n",
    "Ordinary least squares linear regression can be applied to time series data provided the following\n",
    "conditions hold.\n",
    "\n",
    "Assumptions with respect to the behavior of the time series :-\n",
    "- The time series has a linear response to its predictors.\n",
    "- No input variable is constant over time or perfectly correlated with another input variable.\n",
    "\n",
    "Assumptions with respect to the error :-\n",
    "- For each point in time, the expected value of the error, given all explanatory variables for all time periods (forward and backward), is 0.\n",
    "- The error at any given time period is uncorrelated with the inputs at any time period in the past or future. So, a plot of the autocorrelation function of the errors will not indicate any pattern.\n",
    "- Variance of the error is independent of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df737ec2",
   "metadata": {},
   "source": [
    "### 2) Huber Regression\n",
    "It's loss function combines both L1 and L2 panelty. It is robust to outliers. For more information, https://towardsdatascience.com/generalized-huber-regression-505afaff24c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01347eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber = HuberRegressor().fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = huber.predict(test_X)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d2370",
   "metadata": {},
   "source": [
    "### 3) Elastic Net Regression \n",
    "It is a hybrid of Ridge and Lasso Regression. For more information, https://towardsdatascience.com/whats-the-difference-between-linear-regression-lasso-ridge-and-elasticnet-8f997c60cf29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElasticNet().fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88913a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd93c1a",
   "metadata": {},
   "source": [
    "### 4) Passive Aggressive Regression\n",
    "The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C. For more information, https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50682ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PassiveAggressiveRegressor().fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556b296",
   "metadata": {},
   "source": [
    "### 5) Stochastic Gradient Descent Regression\n",
    "Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. SGD is sensitive to feature scaling. For more information, https://scikit-learn.org/stable/modules/sgd.html#sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(StandardScaler(), SGDRegressor()).fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54266dcc",
   "metadata": {},
   "source": [
    "### 6) Ridge Regression\n",
    "This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336033d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge().fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ba87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d317a2",
   "metadata": {},
   "source": [
    "### 7) Lasso Regression\n",
    "Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The acronym “LASSO” stands for Least Absolute Shrinkage and Selection Operator. For more information, https://www.statisticshowto.com/lasso-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d551fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.Lasso().fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6389057",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da355f19",
   "metadata": {},
   "source": [
    "### 8) LassoLars\n",
    " least-angle regression (LARS) is an algorithm for fitting linear regression models to high-dimensional data. For more information, https://en.wikipedia.org/wiki/Least-angle_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb12499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LassoLars().fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b949394",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X)\n",
    "plot()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
